{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drg72ql3nCDR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fqa9l1uZIHJ1",
        "outputId": "8e3f9ac6-28b4-4560-dfd2-4e2d590b22ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import random\n",
        "import ast\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/drive/MyDrive/NLP'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDa-kn1EISZn",
        "outputId": "1dbb0771-874c-455f-e639-ded2581661ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP/sample_submission.csv\n",
            "/content/drive/MyDrive/NLP/test_small.csv\n",
            "/content/drive/MyDrive/NLP/train.csv\n",
            "/content/drive/MyDrive/NLP/sample_submission_finalll.csv\n",
            "/content/drive/MyDrive/NLP/sample_submissionfinal001.csv\n",
            "/content/drive/MyDrive/NLP/sample_submissionfinal002.csv\n",
            "/content/drive/MyDrive/NLP/sample_submissionfinal003.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/NLP/train.csv') # loading training data\n",
        "data = []\n",
        "for index, row in tqdm(df.iterrows()):\n",
        "    data.append(ast.literal_eval(row['tagged_sentence'])) # changing data-type of entries from 'str' to 'list'"
      ],
      "metadata": {
        "id": "_CFJXVAnIeQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72c756e7-6b35-4ade-d94f-f3e322fcca7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "47340it [00:14, 3174.82it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[:5])"
      ],
      "metadata": {
        "id": "X0ryi_WhEXf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b124557f-f518-4b99-bb64-f057b127813c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('The', 'AT'), ('jury', 'NN'), ('further', 'RB'), ('said', 'VB'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NN'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN'), ('Executive', 'JJ'), ('Committee', 'NN'), (',', ','), ('which', 'WD'), ('had', 'HV'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VB'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NN'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN'), ('of', 'IN'), ('Atlanta', 'NP'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WD'), ('the', 'AT'), ('election', 'NN'), ('was', 'BE'), ('conducted', 'VB'), ('.', '.')], [('The', 'AT'), ('September-October', 'NP'), ('term', 'NN'), ('jury', 'NN'), ('had', 'HV'), ('been', 'BE'), ('charged', 'VB'), ('by', 'IN'), ('Fulton', 'NP'), ('Superior', 'JJ'), ('Court', 'NN'), ('Judge', 'NN'), ('Durwood', 'NP'), ('Pye', 'NP'), ('to', 'TO'), ('investigate', 'VB'), ('reports', 'NN'), ('of', 'IN'), ('possible', 'JJ'), ('``', '``'), ('irregularities', 'NN'), (\"''\", \"''\"), ('in', 'IN'), ('the', 'AT'), ('hard-fought', 'JJ'), ('primary', 'NN'), ('which', 'WD'), ('was', 'BE'), ('won', 'VB'), ('by', 'IN'), ('Mayor-nominate', 'NN'), ('Ivan', 'NP'), ('Allen', 'NP'), ('Jr.', 'NP'), ('.', '.')], [('``', '``'), ('Only', 'RB'), ('a', 'AT'), ('relative', 'JJ'), ('handful', 'NN'), ('of', 'IN'), ('such', 'JJ'), ('reports', 'NN'), ('was', 'BE'), ('received', 'VB'), (\"''\", \"''\"), (',', ','), ('the', 'AT'), ('jury', 'NN'), ('said', 'VB'), (',', ','), ('``', '``'), ('considering', 'IN'), ('the', 'AT'), ('widespread', 'JJ'), ('interest', 'NN'), ('in', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('the', 'AT'), ('number', 'NN'), ('of', 'IN'), ('voters', 'NN'), ('and', 'CC'), ('the', 'AT'), ('size', 'NN'), ('of', 'IN'), ('this', 'DT'), ('city', 'NN'), (\"''\", \"''\"), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('said', 'VB'), ('it', 'PP'), ('did', 'DO'), ('find', 'VB'), ('that', 'CS'), ('many', 'AP'), ('of', 'IN'), (\"Georgia's\", 'NP'), ('registration', 'NN'), ('and', 'CC'), ('election', 'NN'), ('laws', 'NN'), ('``', '``'), ('are', 'BE'), ('outmoded', 'JJ'), ('or', 'CC'), ('inadequate', 'JJ'), ('and', 'CC'), ('often', 'RB'), ('ambiguous', 'JJ'), (\"''\", \"''\"), ('.', '.')], [('It', 'PP'), ('recommended', 'VB'), ('that', 'CS'), ('Fulton', 'NP'), ('legislators', 'NN'), ('act', 'VB'), ('``', '``'), ('to', 'TO'), ('have', 'HV'), ('these', 'DT'), ('laws', 'NN'), ('studied', 'VB'), ('and', 'CC'), ('revised', 'VB'), ('to', 'IN'), ('the', 'AT'), ('end', 'NN'), ('of', 'IN'), ('modernizing', 'VB'), ('and', 'CC'), ('improving', 'VB'), ('them', 'PP'), (\"''\", \"''\"), ('.', '.')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/NLP/test_small.csv') # loading test data\n",
        "test_data = {}\n",
        "for index, row in tqdm(df.iterrows()):\n",
        "    test_data[row['id']] = ast.literal_eval(row['untagged_sentence']) # changing data-type of entries from 'str' to 'list'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0Fe9oh5Iowa",
        "outputId": "b925fb24-6318-48fa-9ea9-6a29dff65462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4000it [00:00, 8523.22it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_data(sentence_index):\n",
        "    '''\n",
        "        Input : 'sentence_index' (int) -> index of a sentence in training data\n",
        "        Output: None\n",
        "    '''\n",
        "    sentence = data[sentence_index]\n",
        "    print(\"TOKEN -> TAG\")\n",
        "    print('...')\n",
        "    for token, tag in sentence:\n",
        "        print(token, '>', tag)\n",
        "sentence_index = random.choice(range(len(data)))\n",
        "display_data(sentence_index)"
      ],
      "metadata": {
        "id": "RfcCqsPjIwCT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5090b40-838b-4c71-8402-cb6d197bc249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOKEN -> TAG\n",
            "...\n",
            "Though > CS\n",
            "it > PP\n",
            "may > MD\n",
            "exist > VB\n",
            "in > IN\n",
            "either > CC\n",
            "literate > JJ\n",
            "or > CC\n",
            "illiterate > JJ\n",
            "societies > NN\n",
            ", > ,\n",
            "it > PP\n",
            "assumes > VB\n",
            "a > AT\n",
            "role > NN\n",
            "of > IN\n",
            "true > JJ\n",
            "cultural > JJ\n",
            "importance > NN\n",
            "only > RB\n",
            "in > IN\n",
            "the > AT\n",
            "latter > AP\n",
            ". > .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cell to show the frequency of each distinct (slack or native) present in the training data\n",
        "from collections import Counter\n",
        "distinct_tags = []\n",
        "word_tags = []\n",
        "def store_tags():\n",
        "\n",
        "    global distinct_tags\n",
        "    global word_tags\n",
        "\n",
        "    for sent in data:\n",
        "        word_tags.append(('START','START'))\n",
        "        for words, tag in sent:\n",
        "            word_tags.extend([(tag, words)])\n",
        "        word_tags.append(('END','END'))\n",
        "\n",
        "store_tags()\n",
        "tags=[]\n",
        "for tag, words in word_tags:\n",
        "    tags.append(tag)\n",
        "distinct_tags=list(set(tags))\n",
        "count_tags = {}\n",
        "for tag, count in Counter(tags).items():\n",
        "    count_tags[tag] = count"
      ],
      "metadata": {
        "id": "QRNcniSIIzNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting tagged words from the 'data' dataset\n",
        "tagged_words_list = [tup for sentence in data for tup in sentence]\n",
        "\n",
        "len(tagged_words_list)"
      ],
      "metadata": {
        "id": "s906kjD3I1ma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "495789a4-5c2a-458b-9410-76c47b6ea0b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "957849"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of words by extracting the first element from each tuple in train_tagged_words\n",
        "tokens = [pair[0] for pair in tagged_words_list]\n",
        "\n",
        "# Displaying the first 10 elements of the tokens list\n",
        "print(tokens[:10])\n"
      ],
      "metadata": {
        "id": "1ukME7TyI7AM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c1f2f65-92ac-4fbe-d467-57e9b184707c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'jury',\n",
              " 'further',\n",
              " 'said',\n",
              " 'in',\n",
              " 'term-end',\n",
              " 'presentments',\n",
              " 'that',\n",
              " 'the',\n",
              " 'City']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#vocabulary\n",
        "V = set(tokens)\n",
        "print(len(V))"
      ],
      "metadata": {
        "id": "x2fj8kJwI897",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9411c882-786c-4046-913e-1e2231d4fd85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of tags\n",
        "T = set([pair[1] for pair in tagged_words_list])\n",
        "len(T)"
      ],
      "metadata": {
        "id": "Fd1V5oNkI-y4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c82973f3-c147-4d98-8048-eb63b7ec8a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####EMISSION PROBABILITIES"
      ],
      "metadata": {
        "id": "4YyUHRwdJAKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computing P(w/t) and storing in T x V matrix\n",
        "t = len(T)\n",
        "v = len(V)\n",
        "w_given_t = np.zeros((t, v))"
      ],
      "metadata": {
        "id": "d3ExqYluJCW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute word given tag: Emission Probability\n",
        "\n",
        "def word_given_tag(word, tag, tagged_words_list):\n",
        "    # Filter the tagged words list to get pairs of words and tags with the specified tag\n",
        "    tag_list = [(w, t) for w, t in tagged_words_list if t == tag]\n",
        "\n",
        "    # Count the occurrences of the specified tag\n",
        "    count_tag = len(tag_list)\n",
        "\n",
        "    # Filter the tag_list to get pairs where the word matches the specified word\n",
        "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0] == word]\n",
        "\n",
        "    # Count the occurrences where the word matches the specified word and tag\n",
        "    count_w_given_tag = len(w_given_tag_list)\n",
        "\n",
        "    return count_w_given_tag, count_tag\n"
      ],
      "metadata": {
        "id": "56GCYBEtJD0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_given_tag('city', 'NN'))\n",
        "print(word_given_tag('further', 'VB'))"
      ],
      "metadata": {
        "id": "J5OJlZQDJFiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b0742b5-1037-40ad-b1c5-90059231e060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(212, 187719)\n",
            "(6, 95801)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n",
        "\n",
        "def t2_given_t1(t2, t1,tagged_words_list):\n",
        "    # Extract the list of tags from the tagged words list\n",
        "    tags = [pair[1] for pair in tagged_words_list]\n",
        "\n",
        "    # Count the occurrences of t1 in the tags list\n",
        "    count_t1 = tags.count(t1)\n",
        "\n",
        "    # Initialize a counter for occurrences of t2 following t1\n",
        "    count_t2_t1 = 0\n",
        "\n",
        "    # Iterate through the tags list and check for occurrences of t2 following t1\n",
        "    for index in range(len(tags) - 1):\n",
        "        if tags[index] == t1 and tags[index + 1] == t2:\n",
        "            count_t2_t1 += 1\n",
        "\n",
        "    return count_t2_t1, count_t1\n"
      ],
      "metadata": {
        "id": "Ysvb6T2nJHcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_INVL4Nr28NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating t x t transition matrix of tags\n",
        "# each column is t2, each row is t1\n",
        "# thus M(i, j) represents P(tj given ti)\n",
        "\n",
        "tags_matrix = np.zeros((len(T), len(T)), dtype='float32')\n",
        "for i, t1 in enumerate(list(T)):\n",
        "   for j, t2 in enumerate(list(T)):\n",
        "       tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]"
      ],
      "metadata": {
        "id": "N176rBwDJJku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the matrix to a dataframe df\n",
        "tags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))"
      ],
      "metadata": {
        "id": "gFhBzbM9JMLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags_df"
      ],
      "metadata": {
        "id": "QrBxgL1eNhej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding Tag occurance probability weights"
      ],
      "metadata": {
        "id": "mzaoDgt57GJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list to store tuples of POS tags and their occurrence probabilities based on training data\n",
        "tag_probabilities = []\n",
        "\n",
        "# Calculate the total number of tags in the training data\n",
        "total_tags = len([tag for word, tag in tagged_words_list])\n",
        "\n",
        "# Iterate through each unique POS tag in the training data\n",
        "for current_tag in tags:\n",
        "    # Count the occurrences of the current POS tag in the training data\n",
        "    tag_occurrences = [tag for word, tag in tagged_words_list if tag == current_tag]\n",
        "\n",
        "    # Calculate the occurrence probability of the current POS tag\n",
        "    probability = len(tag_occurrences) / total_tags\n",
        "\n",
        "    # Append a tuple containing the POS tag and its occurrence probability to the list\n",
        "    tag_probabilities.append((current_tag, probability))\n"
      ],
      "metadata": {
        "id": "cTkkbW5YNppD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vanilla_viterbi(input_words, training_bag=tagged_words_list):\n",
        "    # List to store predicted states for each word in the input sequence\n",
        "    predicted_states = []\n",
        "\n",
        "    # Extract unique POS tags from the training data\n",
        "    tag_set = list(set([pair[1] for pair in training_bag]))\n",
        "\n",
        "    # Iterate through each word in the input sequence\n",
        "    for index, word in enumerate(input_words):\n",
        "        # Initialize a list for the probability column for the current observation\n",
        "        probabilities = []\n",
        "\n",
        "        for tag in tag_set:\n",
        "            # Calculate transition probability\n",
        "            if index == 0:\n",
        "                transition_probability = tags_df.loc['.', tag]\n",
        "            else:\n",
        "                transition_probability = tags_df.loc[predicted_states[-1], tag]\n",
        "\n",
        "            # Compute emission and state probabilities\n",
        "            emission_prob = word_given_tag(input_words[index], tag)[0] / word_given_tag(input_words[index], tag)[1]\n",
        "            state_probability = emission_prob * transition_probability\n",
        "            probabilities.append(state_probability)\n",
        "\n",
        "        # Find the maximum probability and corresponding POS tag\n",
        "        max_prob = max(probabilities)\n",
        "        state_max = tag_set[probabilities.index(max_prob)]\n",
        "        predicted_states.append(state_max)\n",
        "\n",
        "    # Combine input words with their predicted POS tags and return the result\n",
        "    return list(zip(input_words, predicted_states))\n"
      ],
      "metadata": {
        "id": "7Ppxtw1r5NXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##First modification in viterbi: to handle unknown words\n",
        "#emission probability for unknown word is zero.\n",
        "#assign only based on transition probabilities."
      ],
      "metadata": {
        "id": "ePZosHpr6KbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a61kL1m27B2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Viterbi_1(words, tagged_words_list, tags_df, word_given_tag, tag_probabilities):\n",
        "    # Initialize the state list to store the predicted POS tags\n",
        "    state = []\n",
        "\n",
        "    # Get the unique POS tags from the training data\n",
        "    T = list(set([pair[1] for pair in tagged_words_list]))\n",
        "\n",
        "    # Iterate through each word in the input sentence\n",
        "    for key, word in enumerate(words):\n",
        "        # Initialise lists for probability and transition probability for each tag\n",
        "        p = []  # list for storing emission probabilities\n",
        "        p_transition = []  # list for storing weighted transition probabilities\n",
        "\n",
        "        # Iterate through each POS tag\n",
        "        for tag in T:\n",
        "            # Compute transition probability based on the previous POS tag\n",
        "            if key == 0:\n",
        "                transition_p = tags_df.loc['.', tag]\n",
        "            else:\n",
        "                transition_p = tags_df.loc[state[-1], tag]\n",
        "\n",
        "            # Compute emission and state probabilities for the current word and tag\n",
        "            emission_p = word_given_tag(word, tag)[0] / word_given_tag(word, tag)[1]\n",
        "            state_probability = emission_p * transition_p\n",
        "            p.append(state_probability)\n",
        "\n",
        "            # Find POS tag occurrence probability\n",
        "            tag_p = [pair[1] for pair in tag_probabilities if pair[0] == tag]\n",
        "\n",
        "            # Calculate the transition probability weighted by tag occurrence probability\n",
        "            if tag_p:\n",
        "                transition_p *= tag_p[0]\n",
        "            p_transition.append(transition_p)\n",
        "\n",
        "        # Choose the POS tag with the maximum emission probability\n",
        "        pmax = max(p)\n",
        "        state_max = T[p.index(pmax)]\n",
        "\n",
        "        # If emission probability is zero (unknown word), use weighted transition probability\n",
        "        if pmax == 0:\n",
        "            pmax = max(p_transition)\n",
        "            state_max = T[p_transition.index(pmax)]\n",
        "\n",
        "        # Append the predicted POS tag to the state list\n",
        "        state.append(state_max)\n",
        "\n",
        "    # Return the list of tuples containing words and their corresponding predicted POS tags\n",
        "    return list(zip(words, state))\n"
      ],
      "metadata": {
        "id": "qoJrBlWpNuF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def memm_features(sentence, index):\n",
        "    word = sentence[index]\n",
        "    return {\n",
        "        'word': word,\n",
        "        'is_first': index == 0,\n",
        "        'is_last': index == len(sentence) - 1,\n",
        "        'is_capitalized': word[0].upper() == word[0],\n",
        "        'has_hyphen': '-' in word,\n",
        "        'is_numeric': word.isdigit(),\n",
        "        'prefix_2': word[:2],  # Prefix of length 2\n",
        "        'prefix_3': word[:3],  # Prefix of length 3\n",
        "        'suffix_2': word[-2:],  # Suffix of length 2\n",
        "        'suffix_3': word[-3:],  # Suffix of length 3\n",
        "        'prev_word': '' if index == 0 else sentence[index - 1],  # Previous word\n",
        "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],  # Next word\n",
        "        'contains_digits': any(char.isdigit() for char in word),  # Check if the word contains digits\n",
        "        'word_length': len(word),  # Length of the word\n",
        "        'is_alphanumeric': word.isalnum(),  # Check if the word is alphanumeric\n",
        "    }\n"
      ],
      "metadata": {
        "id": "TVctEaWKnZAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def viterbi_memm(sequence, hidden_states, model_weights):\n",
        "\n",
        "dynamic_table = [{}]  #stores probability of different states at each time step\n",
        "optimal_path = {}   #stores optimal path for each state at each time step\n",
        "\n",
        "# Initialization at time step 0\n",
        "for state in hidden_states:\n",
        "    dynamic_table[0][state] = model_weights.get(state, 1e-10)\n",
        "    optimal_path[state] = [state]\n",
        "\n",
        "# Recursion step\n",
        "for time_step in range(1, len(sequence)):\n",
        "    dynamic_table.append({})  # stores the probabilities of different states at the current time step\n",
        "    new_optimal_path = {} # stores the updated optimal paths for each state at the current time step\n",
        "    for current_state in hidden_states:\n",
        "        # For each state at the current time step, find the maximum probability and the corresponding previous state\n",
        "        (probability, previous_state) = max([(dynamic_table[time_step-1][prev_state] * model_weights.get(current_state, 1e-10), prev_state) for prev_state in hidden_states])\n",
        "        dynamic_table[time_step][current_state] = probability\n",
        "        new_optimal_path[current_state] = optimal_path[previous_state] + [current_state]\n",
        "    optimal_path = new_optimal_path\n",
        "\n",
        "# Termination step\n",
        "(max_probability, final_state) = max([(dynamic_table[len(sequence) - 1][state], state) for state in hidden_states])\n",
        "return (max_probability, optimal_path[final_state])\n",
        "\n"
      ],
      "metadata": {
        "id": "RldY3j-Rq3Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = {'id': [], 'tagged_sentence' : []} # dictionary to store tag predictions\n",
        "# NOTE ---> ensure that tagged_sentence's corresponing 'id' is same as 'id' of corresponding 'untagged_sentence' in training data\n",
        "def store_submission(sent_id, tagged_sentence):\n",
        "\n",
        "    global submission\n",
        "    submission['id'].append(sent_id)\n",
        "    submission['tagged_sentence'].append(tagged_sentence)\n",
        "\n",
        "def clear_submission():\n",
        "    global submission\n",
        "    submission = {'id': [], 'tagged_sentence' : []}"
      ],
      "metadata": {
        "id": "Y8Bc88qeN9A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hmm_tagger_util(sent_id, untagged_sentence):\n",
        "    # Using Viterbi Heuristic\n",
        "    tagged_sentence = Viterbi_1(untagged_sentence)\n",
        "    store_submission(sent_id, tagged_sentence)"
      ],
      "metadata": {
        "id": "vB1crJxoOK3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sent_id in tqdm(list(test_data.keys())):\n",
        "    sent = test_data[sent_id]\n",
        "    hmm_tagger_util(sent_id, sent)"
      ],
      "metadata": {
        "id": "bgP01AnGOade",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ba132a6-6b19-49ee-ed03-50e313759e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4000/4000 [3:12:37<00:00,  2.89s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(submission).to_csv('sample_submissionfinal1.csv', index = False)"
      ],
      "metadata": {
        "id": "t50n8bX6OeLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GS1kXvMPhIFL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}